{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "724fd965-a913-4827-a03a-e9daddd1146a",
   "metadata": {},
   "source": [
    "# Indexing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181635a8-3d78-4928-8eee-75ec72c16852",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fd5627-231d-4d2b-8e9d-6ecbd4a0c8d1",
   "metadata": {},
   "source": [
    "So far we have moved through the data loading stage, which has involved both retrieving our data (whether pdfs or html), and then chunking that data before ultimately storing it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db60d12-19d6-4560-8461-027886d51f7d",
   "metadata": {},
   "source": [
    "```python\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "\n",
    "\n",
    "documents = [Document(text = doc_text)]\n",
    "parser = SentenceSplitter(chunk_size=1024)\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "\n",
    "index = VectorStoreIndex(nodes)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ba775f-da65-4172-8129-7c4afdc7251f",
   "metadata": {},
   "source": [
    "Now as you can see, in that last step we stored the our nodes in the database.  And as you may remember, we would ultimately retrieve the most relevant nodes (ie. chunks) to our query embedding, and do so by using a similarity score.\n",
    "\n",
    "But going through each individual node and calculating the similarity to the query vector can be time consuming.  So instead we can avoid calculating a similarity for each chunk to our question by determining how we index our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e189f8d-eb1d-401e-bcc5-229440db49ab",
   "metadata": {},
   "source": [
    "### Storage vs Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed241fe-40c3-4013-a898-d45449d77ba3",
   "metadata": {},
   "source": [
    "One thing to note is that a vector store is different from a vector index.  **Storage** is the kind of database that where the nodes are stored.  This can include the [pinecone vector database](www.pinecone.io/), [Neo4J](https://neo4j.com/) or even postgres (with it's pgvector library).  Indexing is the strategy of storing these vectors.  \n",
    "\n",
    "The llamaindex library can be confusing, because when we call the VectorStoreIndex() constructor it both creates an index and will create an in-memory storage if not otherwise specified.\n",
    "\n",
    "```python\n",
    "index = VectorStoreIndex(nodes)\n",
    "# this creates both the index, and an in memory simple vector store\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412797bc-13d7-4f51-ae30-416d20cf27e5",
   "metadata": {},
   "source": [
    "### Indexing strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047584ca-20ba-496b-876b-9efa32e359b8",
   "metadata": {},
   "source": [
    "1. Flat indexing \n",
    "\n",
    "With flat indexing, we just store each vector as is, and when a query arrives, we then calculate distance from the query vector to every stored vector.  The benefit of flat indexing is that it's simple, easy to implement, and provides perfect accuracy. The downside is it is slow, as when a query comes in, a calculation is performed for each stored vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3fbd35-9ad6-4140-9383-23ac7674f42f",
   "metadata": {},
   "source": [
    "```python\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "# Load documents and build index\n",
    "documents = SimpleDirectoryReader(\n",
    "    \"../../examples/data/paul_graham\"\n",
    ").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dbda51-55e1-435b-8f5b-76455fd124a2",
   "metadata": {},
   "source": [
    "The above VectorStoreIndex uses just a flat indexing strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2837bf0-fee1-4402-8f1a-deb832042826",
   "metadata": {},
   "source": [
    "### Keyword Table Index\n",
    "\n",
    "<img src=\"./keyword-table-index.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be8fc3d-d6ec-4db1-af1b-5eed3e8b0252",
   "metadata": {},
   "source": [
    "Here, you can see that each node is tagged with certain keywords.  With this strategy, during query time, the index will extract relevant keywords from the query, and then match the keywords with the already extracted Node keywords.  Then it will return the corresponding nodes to be synthesized by the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8148f1e-c154-4c91-abe4-66a6a7003e9c",
   "metadata": {},
   "source": [
    "```python\n",
    "from llama_index import GPTKeywordTableIndex\n",
    "index = GPTKeywordTableIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is net operating income?\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7826b99c-ed80-4882-845e-02486990d3a0",
   "metadata": {},
   "source": [
    "For more information, please read through the following article:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fdaaf8-91ad-44e9-96a4-3d29921526b3",
   "metadata": {},
   "source": [
    "[Medium Article](https://betterprogramming.pub/llamaindex-how-to-use-index-correctly-6f928b8944c6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57697d92-c907-4d6d-8609-21e38950d90e",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed46cd0a-beef-4e3c-b1f6-e5bdbe24f3de",
   "metadata": {},
   "source": [
    "[Medium LLamaindex](https://betterprogramming.pub/llamaindex-how-to-use-index-correctly-6f928b8944c6)\n",
    "\n",
    "[DataStax - VectorIndex](https://www.datastax.com/guides/what-is-a-vector-index)\n",
    "\n",
    "[Llamaindex](https://docs.llamaindex.ai/en/stable/module_guides/indexing/index_guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae209db-89f1-4056-a760-da47be08683a",
   "metadata": {},
   "source": [
    "[LLamaindex - other resources](https://docs.llamaindex.ai/en/stable/module_guides/indexing/indexing.html#other-index-resources)\n",
    "\n",
    "* See also the FAISS library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6469712a-3869-4c74-a3b1-8eb27f1cdf1f",
   "metadata": {},
   "source": [
    "[Building a Simple Vector Store](https://docs.llamaindex.ai/en/stable/examples/low_level/vector_store.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
