{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "724fd965-a913-4827-a03a-e9daddd1146a",
   "metadata": {},
   "source": [
    "# Indexing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181635a8-3d78-4928-8eee-75ec72c16852",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fd5627-231d-4d2b-8e9d-6ecbd4a0c8d1",
   "metadata": {},
   "source": [
    "So far we have moved through the data loading stage, which has involved both retrieving our data (whether pdfs or html), and then chunking that data before ultimately storing it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db60d12-19d6-4560-8461-027886d51f7d",
   "metadata": {},
   "source": [
    "```python\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "\n",
    "\n",
    "documents = [Document(text = doc_text)]\n",
    "parser = SentenceSplitter(chunk_size=1024)\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "\n",
    "index = VectorStoreIndex(nodes)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ba775f-da65-4172-8129-7c4afdc7251f",
   "metadata": {},
   "source": [
    "Now as you can see, in that last step we stored the our nodes in the database.  And as you may remember, we would ultimately retrieve the most relevant nodes (ie. chunks) to our query embedding, and do so by using a similarity score.\n",
    "\n",
    "But going through each individual node and calculating the similarity to the query vector can be time consuming.  So instead we can avoid calculating a similarity for each chunk to our question by determining how we index our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e189f8d-eb1d-401e-bcc5-229440db49ab",
   "metadata": {},
   "source": [
    "### Storage vs Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed241fe-40c3-4013-a898-d45449d77ba3",
   "metadata": {},
   "source": [
    "One thing to note is that a vector store is different from a vector index.  **Storage** is the kind of database that where the nodes are stored.  This can include the [pinecone vector database](www.pinecone.io/), [Neo4J](https://neo4j.com/) or even postgres (with it's pgvector library).  Indexing is the strategy of storing these vectors.  \n",
    "\n",
    "The llamaindex library can be confusing, because when we call the VectorStoreIndex() constructor it both creates an index and will create an in-memory storage if not otherwise specified.\n",
    "\n",
    "```python\n",
    "index = VectorStoreIndex(nodes)\n",
    "# this creates both the index, and an in memory simple vector store\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412797bc-13d7-4f51-ae30-416d20cf27e5",
   "metadata": {},
   "source": [
    "### Indexing strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047584ca-20ba-496b-876b-9efa32e359b8",
   "metadata": {},
   "source": [
    "1. VectorStoreIndex \n",
    "\n",
    "With the vector store index, when we pass through or generate our nodes, the VectorStoreIndex will call OpenAI's embeddings API to generate an embedding for each node.  When a query arrives, the index will similarity between the query vector and every vector in the index.  It then sends the most relevant documents to the llm.\n",
    "\n",
    "The benefit of the vector store indexing is that it's simple, easy to implement, and provides perfect accuracy. The downside is it is slow, as when a query comes in, a calculation is performed for each stored vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3fbd35-9ad6-4140-9383-23ac7674f42f",
   "metadata": {},
   "source": [
    "```python\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "# Load documents and build index\n",
    "documents = SimpleDirectoryReader(\n",
    "    \"../../examples/data/paul_graham\"\n",
    ").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dbda51-55e1-435b-8f5b-76455fd124a2",
   "metadata": {},
   "source": [
    "This strategy of simply storing each of the nodes and then calculating a similarity score for each node is also known as a flat indexing strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2837bf0-fee1-4402-8f1a-deb832042826",
   "metadata": {},
   "source": [
    "### Keyword Table Index\n",
    "\n",
    "<img src=\"./keyword-table-index.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be8fc3d-d6ec-4db1-af1b-5eed3e8b0252",
   "metadata": {},
   "source": [
    "Here, you can see that each node is tagged with certain keywords. \n",
    "\n",
    "With this strategy, during query time, the index will extract relevant keywords from the query, and then match the keywords with the already extracted Node keywords.  Then it will return the corresponding nodes to be synthesized by the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8148f1e-c154-4c91-abe4-66a6a7003e9c",
   "metadata": {},
   "source": [
    "```python\n",
    "from llama_index import GPTKeywordTableIndex\n",
    "index = GPTKeywordTableIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is net operating income?\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d173bb07-df3c-4f4d-a849-8d2a46ca7e83",
   "metadata": {},
   "source": [
    "A benefit of this strategy is that it's faster to retrieve the relevant nodes.  However, a downside is that to tag nodes with keywords, if using OpenAI, the more expensive Completion API is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d416e1-b9fa-4604-b8e8-b188a28597fc",
   "metadata": {},
   "source": [
    "If you run the index.py file in the codebase, you can see the results of using our KeywordIndex.  \n",
    "\n",
    "```python\n",
    "keyword_index = GPTKeywordTableIndex.from_documents(documents)\n",
    "```\n",
    "\n",
    "Then we call `keyword_index.index_struct` to see the resulting shape of the index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5dfb7b-8b5a-4cf0-bd0f-c1813502c12f",
   "metadata": {},
   "source": [
    "<img src=\"./keyword-struct.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d924a8-847b-45b6-a50d-7229477f00cc",
   "metadata": {},
   "source": [
    "Notice that `table` is a dictionary, where the keys are the keywords and the value is a set of nodes.\n",
    "\n",
    "`{'joseph carrubba': {}, 'francisco': {}}`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e693ae1e-906e-4328-9eb0-66984c73c18d",
   "metadata": {},
   "source": [
    "And from there if we would like to find the node associated with each keyword, we can do so by accessing that node from the index's docstore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7596d4ec-3b14-44d6-80aa-4b22da271292",
   "metadata": {},
   "source": [
    "```python\n",
    "node_id = 'b113c64b-873c-4936-83f2-207c002f8136'\n",
    "keyword_index.docstore.docs[node_id].text\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db987a40-6a58-4b7c-91e4-2f8460b198a1",
   "metadata": {},
   "source": [
    "> One thing you may notice is that we are indexing some information from our pdf that may be pretty irrelevant -- like the list of sources at the end.  This may be a case only feeding part of the pdf to our database with some up front data engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1744f209-e51f-45ab-9d26-fd7c9e707b37",
   "metadata": {},
   "source": [
    "### Tree index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b757fcaa-912b-4ff6-9f18-0ea4eaae251c",
   "metadata": {},
   "source": [
    "So the Keyword Index generates a list of keywords, and then associates nodes to these keywords.  The tree index builds on this idea, has nodes of text summaries, and then these summaries point to the original document nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71efd5ca-002b-4cd8-84e5-c98a8548eeb0",
   "metadata": {},
   "source": [
    "```python\n",
    "from llama_index import GPTTreeIndex\n",
    "\n",
    "tree_index = GPTTreeIndex.from_documents(documents)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83087f5b-0b42-4791-ba4c-32944ab82248",
   "metadata": {},
   "source": [
    "For example, in this case if we look at the index_struct, we will see that one node points to child nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e903f6-3ffe-4c8b-ae2b-ca8085a9b920",
   "metadata": {},
   "source": [
    "```python\n",
    "{'14988bff-bc2a-448f-95a6-5c60b18a3f94': ['b986e069-40e1-44bb-bd07-80e7528ea178', '722c6520-3da2-4c3d-aed7-e87b8e1d3005', 'fcbf63d5-1975-4381-969c-68ae2eaa1400']}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0dd410-a02d-4e0e-8a76-1a537e3ccd1c",
   "metadata": {},
   "source": [
    "And this parent node is really a summary derived from those child nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4747f7-c196-48a1-a410-e678cbabf597",
   "metadata": {},
   "source": [
    "<img src=\"./summary-node.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a63db1f-0bea-41ed-9a2b-a81561b97012",
   "metadata": {},
   "source": [
    "So when a query comes in, it will first look at these parent nodes, and then from there traverse to the original nodes from the document, and send those to the llm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7b8f3c-ced4-4b9d-8a30-ea8cf7a1e5d1",
   "metadata": {},
   "source": [
    "<img src=\"./tree-index.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e892ca9e-f896-471e-9ee1-1b9502b1b7f8",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965242bc-9e50-4f83-9201-7b86f3dfd7e3",
   "metadata": {},
   "source": [
    "In this lesson, we learned a few different mechanisms for indexing our data.  As we saw the VectorStore actually stores our data, where with indexing we can change *how* that data is stored.  \n",
    "\n",
    "* VectorIndexStore - Here each node is embedded and stored.  When a query comes in, the similarity on all nodes is calculated.  This is accurate but takes time.\n",
    "\n",
    "* Keyword Index - Only search through those with a matching keyword.\n",
    "\n",
    "* Tree Index - Generates parent summary nodes.  When queried, searches through the nodes matching those summary nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57697d92-c907-4d6d-8609-21e38950d90e",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed46cd0a-beef-4e3c-b1f6-e5bdbe24f3de",
   "metadata": {},
   "source": [
    "[Leeway Hertz](https://www.leewayhertz.com/llamaindex)\n",
    "\n",
    "[Benefits of Different Indexes](https://blog.gopenai.com/different-types-of-indexes-in-llamaindex-to-improve-your-rag-system-0fb13132cab6)\n",
    "\n",
    "[Medium LLamaindex](https://betterprogramming.pub/llamaindex-how-to-use-index-correctly-6f928b8944c6)\n",
    "\n",
    "[DataStax - VectorIndex](https://www.datastax.com/guides/what-is-a-vector-index)\n",
    "\n",
    "[Llamaindex](https://docs.llamaindex.ai/en/stable/module_guides/indexing/index_guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae209db-89f1-4056-a760-da47be08683a",
   "metadata": {},
   "source": [
    "[LLamaindex - other resources](https://docs.llamaindex.ai/en/stable/module_guides/indexing/indexing.html#other-index-resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6469712a-3869-4c74-a3b1-8eb27f1cdf1f",
   "metadata": {},
   "source": [
    "[Building a Simple Vector Store](https://docs.llamaindex.ai/en/stable/examples/low_level/vector_store.html)\n",
    "\n",
    "See also the FAISS library"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
